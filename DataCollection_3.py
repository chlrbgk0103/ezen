import requests
from bs4 import BeautifulSoup
import re
import oracledb as cx_Oracle
import os
import glob

# âœ… ì •ì±… ìƒì„¸ ì •ë³´ ì„¹ì…˜ í¬ë¡¤ë§
def crawl_all_sections(url):
    response = requests.get(url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")

    data_store = {}
    titles = soup.find_all("strong", class_="tit")

    for title in titles:
        section_name = title.get_text(strip=True)
        table = title.find_next("table", class_="form-table form-resp-table")
        if table:
            section_data = {}
            rows = table.find_all("tr")
            for row in rows:
                ths = row.find_all("th")
                tds = row.find_all("td")
                for th, td in zip(ths, tds):
                    key = th.get_text(strip=True)
                    val = td.get_text(" ", strip=True).replace("\xa0", " ")
                    section_data[key] = val
            data_store[section_name] = section_data
    return data_store

# âœ… ì§ˆë¬¸ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
def find_best_section(question, data_store):
    question_lower = question.lower()
    best_section = None
    max_matches = 0

    for section_name in data_store.keys():
        matches = sum(1 for word in section_name.lower().split() if word in question_lower)
        if matches > max_matches:
            max_matches = matches
            best_section = section_name

    return best_section

# âœ… ë‹µë³€ ìƒì„±
def generate_answer(question, data_store):
    section = find_best_section(question, data_store)
    if not section:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    content = data_store.get(section, {})
    answer = f"{section}\n"
    for k, v in content.items():
        answer += f"{k}: {v}\n"
    return answer

# âœ… ì •ì±… ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
def crawl_policy_list(list_url):
    response = requests.get(list_url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")

    policy_items = soup.select("ul.policy-list li")
    policy_data = []

    for item in policy_items:
        try:
            category = item.select_one("span.bg-blue").get_text(strip=True)
            a_tag = item.select_one("a.tit.txt-over1")
            title = a_tag.get_text(strip=True)
            onclick = a_tag.get("onclick", "")
            class_attr = a_tag.get("class", [])

            policy_id = ""
            if "goView" in onclick:
                policy_id = onclick.replace("goView('", "").replace("');", "").strip()

            description = item.select_one("em.txt-over1").get_text(separator=" ", strip=True)

            policy_data.append({
                "category": category,
                "title": title,
                "policy_id": policy_id,
                "a_class": class_attr,
                "description": description
            })
        except Exception as e:
            print("íŒŒì‹± ì˜¤ë¥˜:", e)
            continue

    return policy_data

# âœ… ì •ì±… ID ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
def load_saved_policy_ids_from_files(*file_paths):
    saved_ids = set()
    id_pattern = re.compile(r"plcyBizId=([^&\s]+)")
    for file_path in file_paths:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    match = id_pattern.search(line)
                    if match:
                        saved_ids.add(match.group(1).strip())
        except FileNotFoundError:
            pass
    return saved_ids

# âœ… íŠ¹ìˆ˜ë¬¸ì ì œê±°
def remove_special_chars_with_space(text):
    cleaned = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    cleaned = " ".join(cleaned.split())
    return cleaned

# âœ… file3 ì €ì¥
#def save_policy_result_to_file(file_path, title, questions, data_store):
def save_policy_result_to_file(file_path, title, questions, data_store, detail_url):
    with open(file_path, "a", encoding="utf-8") as f:
        f.write('"""' + title + "\n")
        f.write(detail_url + "\n")  # âœ… URL ì¶”ê°€

        for i, q in enumerate(questions):
            result = (
                generate_answer(q, data_store)
                .replace("\n", " ")
                .replace("\xa0", " ")
                .replace("â–¡", "-")
                .replace("ã…‡", "-")
                .replace("Â·", "-")
                .strip()
            )
            result = remove_special_chars_with_space(result)
            f.write(result + "\n")
        f.write('"""' + "\n")

# âœ… ì „ì²´ ì •ì±… í˜ì´ì§€ ìˆœíšŒ
def crawl_all_policy_pages():
    all_policies = []
    page = 1
    while True:
        list_url = f"https://youth.seoul.go.kr/infoData/plcyInfo/ctList.do?sprtInfoId=&plcyBizId=&key=2309150002&sc_detailAt=&pageIndex={page}&orderBy=regYmd+desc&blueWorksYn=N&tabKind=002&sw=&sc_rcritCurentSitu=001&sc_rcritCurentSitu=002"
        page_policies = crawl_policy_list(list_url)
        if not page_policies:
            print("âŒ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
            break
        all_policies.extend(page_policies)
        page += 1
    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ ì •ì±… ìˆ˜: {len(all_policies)}ê°œ")
    return all_policies

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    #file1_path = "D:/dochoi/workspace/PythonProject1/my_data_directory/your_data_file1.txt"
    base_dir = "D:/dochoi/workspace/PythonProject1/policy_directory/"
    base_file3_name = os.path.join(base_dir, "your_data_file")

    # file3 ê²½ë¡œ íƒìƒ‰ (file1 ì œì™¸)
    file3_paths = [
        p for p in glob.glob(os.path.join(base_dir, "your_data_file*.txt"))
#        if not p.endswith("your_data_file1.txt")
    ]
    print("ğŸ“‚ íƒìƒ‰ëœ file3 ê²½ë¡œë“¤:", file3_paths)

    # file3 ì¸ë±ìŠ¤ ê³„ì‚°
    existing_indexes = []
    for path in file3_paths:
        match = re.search(r"your_data_file(\d+)\.txt", path)
        if match:
            existing_indexes.append(int(match.group(1)))

#    file3_index = max(existing_indexes, default=9) + 1
    file3_index = max(existing_indexes) + 1
    file3_path = f"{base_file3_name}{file3_index}.txt"
    print(f"ğŸ“ ìµœì´ˆ ì €ì¥ íŒŒì¼: {file3_path}")
    save_count = 0

    # ì¤‘ë³µ ì •ì±… ID ë¡œë”©
    #saved_policy_ids = load_saved_policy_ids_from_files(file1_path, *file3_paths)
    saved_policy_ids = load_saved_policy_ids_from_files(*file3_paths)

    all_policies = crawl_all_policy_pages()

    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    test_questions = [
        "ì‚¬ì—…ê°œìš”ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
        "ì‹ ì²­ìê²©ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì‹ ì²­ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”",
        "ê¸°íƒ€ ì •ë³´ê°€ ìˆë‚˜ìš”?",
        "ì§€ì› ë‚´ìš©ì´ ë­”ê°€ìš”?"
    ]

    dsn = cx_Oracle.makedsn("192.168.0.231", 1521, service_name="helperpdb")
    conn = cx_Oracle.connect(user="helper", password="1111", dsn=dsn)
    cursor = conn.cursor()

    inserted_count = 0

    for i, policy in enumerate(all_policies):
        policy_id = policy["policy_id"]
        if not policy_id:
            continue
        if policy_id in saved_policy_ids:
            print(f"[ì¤‘ë³µ - ID ê¸°ì¤€] '{policy_id}' ì´ë¯¸ ì €ì¥ë˜ì–´ ê±´ë„ˆëœ€")
            continue

        detail_url = f"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId={policy_id}&tab=001&key=2309150002"

        try:
            res = requests.get(detail_url)
            res.encoding = 'utf-8'
            soup = BeautifulSoup(res.text, "html.parser")
            policy_title = soup.find("strong", class_="title").get_text(strip=True)
            data_store = crawl_all_sections(detail_url)

            # file1 ê¸°ë¡
            #with open(file1_path, "a", encoding="utf-8") as f:
            #    f.write(f"{policy_title}\n{detail_url}\n\n")

            # DB INSERT
            try:
                cursor.execute("INSERT INTO policies (title, url) VALUES (:1, :2)", (policy_title, detail_url))
               # cursor.execute("INSERT INTO policies (title, url) VALUES (:1, :2)", (policy_title, detail_url))
                inserted_count += 1
                print(f"[INSERT ì™„ë£Œ] {policy_title}")
            except cx_Oracle.IntegrityError:
                print(f"[ì¤‘ë³µ - DB ê¸°ì¤€] {policy_title} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœ€")
            except Exception as e:
                print(f"[DB ERROR] ì œëª© : {policy_title} | ì˜¤ë¥˜ : {e}")

            # file3 ë¶„í•  ì €ì¥
            if save_count >= 20:
                file3_index += 1
                file3_path = f"{base_file3_name}{file3_index}.txt"
                save_count = 0

            save_policy_result_to_file(file3_path, policy_title, test_questions, data_store, detail_url)
            print(f"ğŸ“Œ save_count: {save_count} | í˜„ì¬ íŒŒì¼: {file3_path}")

            save_count += 1
            saved_policy_ids.add(policy_id)

        except Exception as e:
            print(f"[{i+1}] ì •ì±… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - ID: {policy_id} / ì˜¤ë¥˜: {e}")
            continue

    conn.commit()
    cursor.close()
    conn.close()

    print(f"\nâœ… ìˆ˜ì§‘ëœ ì „ì²´ ì •ì±… ìˆ˜: {len(all_policies)}ê°œ")
    print(f"ğŸŸ¢ DBì— ì‹¤ì œ INSERT ëœ ì •ì±… ìˆ˜: {inserted_count}ê°œ")
    print("\n----------------------- ë°ì´í„° ì €ì¥ ì™„ë£Œ -----------------------------")
